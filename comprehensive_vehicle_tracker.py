import os
import cv2
import numpy as np
from ultralytics import YOLO
from collections import deque
import time

# Proper SORT Tracker Implementation
class Sort:
    def __init__(self, max_age=30, min_hits=3, iou_threshold=0.3):
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        self.trackers = []
        self.frame_count = 0
        self.next_id = 0
        
    def update(self, detections):
        self.frame_count += 1
        
        # If no detections, update all trackers
        if len(detections) == 0:
            # Update existing trackers and mark as lost
            for tracker in self.trackers:
                if tracker is not None:
                    tracker['age'] += 1
                    tracker['lost'] = True
            return np.empty((0, 5))
        
        # If no trackers exist, create new ones
        if len(self.trackers) == 0:
            for det in detections:
                self._create_tracker(det)
            return np.array([np.append(det, [i]) for i, det in enumerate(detections)])
        
        # Match detections to existing trackers using IoU
        matched_detections = set()
        matched_trackers = set()
        
        for i, detection in enumerate(detections):
            best_iou = 0
            best_tracker_idx = -1
            
            for j, tracker in enumerate(self.trackers):
                if tracker is not None and not tracker['lost']:
                    iou = self._calculate_iou(detection, tracker['bbox'])
                    if iou > best_iou and iou > self.iou_threshold:
                        best_iou = iou
                        best_tracker_idx = j
            
            if best_tracker_idx != -1:
                # Update existing tracker
                self.trackers[best_tracker_idx]['bbox'] = detection
                self.trackers[best_tracker_idx]['age'] += 1
                self.trackers[best_tracker_idx]['lost'] = False
                matched_detections.add(i)
                matched_trackers.add(best_tracker_idx)
        
        # Create new trackers for unmatched detections
        for i, detection in enumerate(detections):
            if i not in matched_detections:
                self._create_tracker(detection)
        
        # Update unmatched trackers
        for j, tracker in enumerate(self.trackers):
            if j not in matched_trackers and tracker is not None:
                tracker['age'] += 1
                tracker['lost'] = True
        
        # Remove old trackers
        self.trackers = [t for t in self.trackers if t is None or t['age'] < self.max_age]
        
        # Return tracked objects
        tracked_objects = []
        for tracker in self.trackers:
            if tracker is not None and not tracker['lost']:
                tracked_objects.append(np.append(tracker['bbox'], [tracker['id']]))
        
        return np.array(tracked_objects) if tracked_objects else np.empty((0, 5))
    
    def _create_tracker(self, detection):
        tracker = {
            'id': self.next_id,
            'bbox': detection,
            'age': 1,
            'lost': False
        }
        self.trackers.append(tracker)
        self.next_id += 1
    
    def _calculate_iou(self, bbox1, bbox2):
        """Calculate Intersection over Union between two bounding boxes"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # Calculate intersection
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Calculate union
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0

# Vehicle class names in COCO dataset
VEHICLE_CLASSES = [2, 3, 5, 7]  # car, motorcycle, bus, truck (COCO IDs)
CLASS_NAME_MAP = {
    2: 'car',
    3: 'motorcycle',
    5: 'bus',
    7: 'truck',
}

def calculate_speed(prev_pos, curr_pos, fps, pixel_to_meter_ratio=0.05):
    """Calculate speed in pixels/frame and km/h with better accuracy"""
    if prev_pos is None:
        return 0, 0
    
    dx = curr_pos[0] - prev_pos[0]
    dy = curr_pos[1] - prev_pos[1]
    distance_pixels = np.sqrt(dx*dx + dy*dy)
    
    # More realistic pixel-to-meter conversion (adjust this based on your video)
    # For typical traffic videos, 1 pixel â‰ˆ 0.05 meters
    distance_meters = distance_pixels * pixel_to_meter_ratio
    
    # Calculate speed in m/s
    speed_mps = distance_meters * fps
    
    # Convert to km/h
    speed_kmh = speed_mps * 3.6
    
    # Apply realistic speed limits (typical urban traffic: 0-80 km/h)
    if speed_kmh > 80:
        speed_kmh = 80  # Cap at reasonable urban speed
    
    return distance_pixels, speed_kmh

def smooth_speed(speeds, window_size=5):
    """Apply moving average smoothing to speed values"""
    if len(speeds) < window_size:
        return speeds
    
    smoothed = []
    for i in range(len(speeds)):
        start = max(0, i - window_size + 1)
        end = i + 1
        window_speeds = speeds[start:end]
        smoothed.append(sum(window_speeds) / len(window_speeds))
    
    return smoothed

def calculate_journey_speed(entry_frame, exit_frame, entry_pos, exit_pos, fps, pixel_to_meter_ratio=0.05):
    """Calculate speed based on total journey through the video"""
    if entry_frame is None or exit_frame is None:
        return 0
    
    # Calculate total time in video (in seconds)
    total_frames = exit_frame - entry_frame
    total_time_seconds = total_frames / fps
    
    # Calculate total distance traveled (in pixels)
    dx = exit_pos[0] - entry_pos[0]
    dy = exit_pos[1] - entry_pos[1]
    total_distance_pixels = np.sqrt(dx*dx + dy*dy)
    
    # Convert to meters
    total_distance_meters = total_distance_pixels * pixel_to_meter_ratio
    
    # Calculate average speed (m/s)
    if total_time_seconds > 0:
        avg_speed_mps = total_distance_meters / total_time_seconds
        avg_speed_kmh = avg_speed_mps * 3.6
        
        # Apply realistic speed limits
        if avg_speed_kmh > 80:
            avg_speed_kmh = 80
    else:
        avg_speed_kmh = 0
    
    return avg_speed_kmh, total_distance_meters, total_time_seconds

def main(input_path):
    print(f"Processing video: {input_path}")
    
    # Check if input video exists
    if not os.path.exists(input_path):
        print(f"Error: Video file not found at {input_path}")
        return
    
    # Create output directories
    os.makedirs('dataset/Output_Data/Object_Detection', exist_ok=True)
    os.makedirs('dataset/Output_Data/Frames_Detected', exist_ok=True)
    os.makedirs('dataset/labels/Frames', exist_ok=True)
    os.makedirs('dataset/Speed', exist_ok=True)
    
    base_name = os.path.splitext(os.path.basename(input_path))[0]
    output_path = f'dataset/Output_Data/Object_Detection/{base_name}_detected.mp4'
    
    # Load YOLOv8 model
    model = YOLO('yolov8n.pt')
    
    # Initialize SORT tracker
    tracker = Sort(max_age=30, min_hits=3, iou_threshold=0.3)
    
    # Open video
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {input_path}")
        return
        
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # Tracking state
    track_history = {}  # track_id: [positions]
    vehicle_classes = {}  # track_id: vehicle_class_name
    vehicle_journey = {}  # track_id: {entry_frame, exit_frame, entry_pos, exit_pos}
    frame_idx = 0
    
    print("Starting processing...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        # 1. EXTRACT AND SAVE FRAME
        frame_filename = f"{base_name}_frame_{frame_idx}.jpg"
        frame_path = os.path.join('dataset/Output_Data/Frames_Detected', frame_filename)
        cv2.imwrite(frame_path, frame)
        
        # 2. OBJECT DETECTION (YOLOv8) - Keep original detection accuracy
        results = model(frame, classes=VEHICLE_CLASSES, conf=0.25)  # Lower confidence threshold for better detection
        detections = []
        detection_classes = []  # Store class information
        detection_confidences = []  # Store confidence scores
        
        if results[0].boxes is not None and results[0].boxes.xyxy is not None:
            boxes = results[0].boxes.xyxy.cpu().numpy()
            classes = results[0].boxes.cls.cpu().numpy().astype(int)
            confidences = results[0].boxes.conf.cpu().numpy()
            
            for box, cls, conf in zip(boxes, classes, confidences):
                detections.append(box)
                detection_classes.append(cls)
                detection_confidences.append(conf)
        
        # 3. OBJECT TRACKING (SORT) - Use SORT only for tracking, not for detection
        tracked_objects = tracker.update(np.array(detections))
        
        # 4. GENERATE LABELS AND DRAW ANNOTATIONS
        yolo_lines = []
        
        # Process tracked objects (maintain YOLO detection accuracy)
        for i, track in enumerate(tracked_objects):
            if len(track) >= 5:  # Ensure we have all required data
                x1, y1, x2, y2, track_id = track
                
                # Convert to integers for drawing
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                track_id = int(track_id)
                
                # Get vehicle class name from original YOLO detection
                vehicle_class = "unknown"
                if i < len(detection_classes):
                    cls_id = detection_classes[i]
                    vehicle_class = CLASS_NAME_MAP.get(cls_id, "unknown")
                
                # Calculate centroid for tracking
                centroid = ((x1 + x2) // 2, (y1 + y2) // 2)
                
                # Update track history and journey tracking
                if track_id not in track_history:
                    track_history[track_id] = deque(maxlen=30)  # Keep last 30 positions
                    vehicle_classes[track_id] = vehicle_class
                    vehicle_journey[track_id] = {
                        'entry_frame': frame_idx,
                        'entry_pos': centroid,
                        'exit_frame': None,
                        'exit_pos': None
                    }
                
                track_history[track_id].append(centroid)
                
                # Update exit position (will be updated each frame until vehicle disappears)
                vehicle_journey[track_id]['exit_frame'] = frame_idx
                vehicle_journey[track_id]['exit_pos'] = centroid
                
                # Draw bounding box with confidence
                color = (0, 255, 0)  # Green
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                
                # Draw unique serial number and vehicle type above the bounding box
                serial_text = f"#{track_id} {vehicle_class}"
                # Position text above the box
                text_x = x1
                text_y = y1 - 10
                
                # Draw serial number and vehicle type text
                cv2.putText(frame, serial_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Generate YOLO format label (for all detected objects)
                x_center = ((x1 + x2) / 2) / width
                y_center = ((y1 + y2) / 2) / height
                w = (x2 - x1) / width
                h = (y2 - y1) / height
                yolo_lines.append(f"0 {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}")
        
        # 5. SAVE LABEL FILE
        label_filename = f"{base_name}_frame_{frame_idx}.txt"
        label_path = os.path.join('dataset/labels/Frames', label_filename)
        with open(label_path, 'w') as f:
            f.write('\n'.join(yolo_lines))
        
        # 6. WRITE FRAME TO OUTPUT VIDEO
        out.write(frame)
        
        frame_idx += 1
        if frame_idx % 30 == 0:
            print(f"Processed {frame_idx} frames...")
    
    # Calculate journey-based speeds and save to file
    speed_file_path = f'dataset/Speed/{base_name}_vehicle_speeds.txt'
    with open(speed_file_path, 'w') as f:
        f.write("Vehicle Serial No | Vehicle Type | Avg Speed (km/h) | Distance (m) | Time (s) | Frames Tracked\n")
        f.write("-" * 95 + "\n")
        
        for track_id, journey in vehicle_journey.items():
            if journey['entry_frame'] is not None and journey['exit_frame'] is not None:
                avg_speed, distance, time = calculate_journey_speed(
                    journey['entry_frame'], 
                    journey['exit_frame'], 
                    journey['entry_pos'], 
                    journey['exit_pos'], 
                    fps
                )
                vehicle_type = vehicle_classes.get(track_id, "unknown")
                total_frames = journey['exit_frame'] - journey['entry_frame']
                f.write(f"{track_id:^18} | {vehicle_type:^12} | {avg_speed:^15.2f} | {distance:^12.2f} | {time:^8.2f} | {total_frames:^15}\n")
    
    # Create final frame with journey-based speeds displayed
    final_frame = np.zeros((height, width, 3), dtype=np.uint8)
    final_frame[:] = (0, 0, 0)  # Black background
    
    # Add title
    cv2.putText(final_frame, f"Vehicle Journey Summary - {base_name}", (50, 50), 
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    y_offset = 100
    for track_id, journey in vehicle_journey.items():
        if journey['entry_frame'] is not None and journey['exit_frame'] is not None:
            avg_speed, distance, time = calculate_journey_speed(
                journey['entry_frame'], 
                journey['exit_frame'], 
                journey['entry_pos'], 
                journey['exit_pos'], 
                fps
            )
            vehicle_type = vehicle_classes.get(track_id, "unknown")
            total_frames = journey['exit_frame'] - journey['entry_frame']
            text = f"Vehicle {track_id} ({vehicle_type}): {avg_speed:.2f} km/h, {distance:.1f}m in {time:.1f}s"
            cv2.putText(final_frame, text, (50, y_offset), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            y_offset += 35
    
    # Save final frame
    final_frame_path = f'dataset/Speed/{base_name}_journey_summary.jpg'
    cv2.imwrite(final_frame_path, final_frame)
    
    # Cleanup
    cap.release()
    out.release()
    
    print(f"Processing complete!")
    print(f"Frames saved to: dataset/Output_Data/Frames_Detected/")
    print(f"Labels saved to: dataset/labels/Frames/")
    print(f"Output video saved to: {output_path}")
    print(f"Journey-based speed data saved to: {speed_file_path}")
    print(f"Journey summary image saved to: {final_frame_path}")
    print(f"Total frames processed: {frame_idx}")

if __name__ == "__main__":
    # Set your input video path here
    input_path = "dataset/input_video/Video1.mp4"
    main(input_path)
